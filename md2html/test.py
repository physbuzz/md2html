#!/usr/bin/env python3
"""
Test suite for md2html - Comprehensive scenario testing
Run with: python -m md2html.test
Generated by Claude Opus 4.1 and Grok 4
"""

import json
import os
import subprocess
import sys
from pathlib import Path
from typing import Dict, List, Tuple, Set

# Colors for terminal output
class Colors:
    GREEN = '\033[92m'
    RED = '\033[91m'
    YELLOW = '\033[93m'
    BLUE = '\033[94m'
    RESET = '\033[0m'
    BOLD = '\033[1m'
    DIM = '\033[2m'

def run_md2html(args: List[str], cwd: Path = None) -> Tuple[bool, str, str]:
    """Run md2html with given arguments and return success, stdout, stderr"""
    cmd = [sys.executable, '-m', 'md2html.md2html'] + args
    
    # Get the project root (parent of md2html package)
    project_root = Path(__file__).parent.parent
    
    # Set up environment with proper PYTHONPATH
    env = os.environ.copy()
    if 'PYTHONPATH' in env:
        env['PYTHONPATH'] = str(project_root) + os.pathsep + env['PYTHONPATH']
    else:
        env['PYTHONPATH'] = str(project_root)
    
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, cwd=cwd, env=env)
        return result.returncode == 0, result.stdout, result.stderr
    except Exception as e:
        return False, "", str(e)

def parse_dag_output(output: str) -> Dict:
    """Parse the JSON DAG output"""
    try:
        return json.loads(output)
    except json.JSONDecodeError:
        return None

def get_output_files(dag: Dict) -> Set[str]:
    """Get set of output files from DAG"""
    nodes = dag.get('nodes', [])
    outputs = set()
    for node in nodes:
        if node['output']:
            # Normalize path for comparison
            outputs.add(Path(node['output']).as_posix())
    return outputs

def get_input_files(dag: Dict) -> Set[str]:
    """Get set of input files from DAG"""
    nodes = dag.get('nodes', [])
    inputs = set()
    for node in nodes:
        # Normalize path for comparison
        inputs.add(Path(node['input']).as_posix())
    return inputs

def check_for_duplicates(dag: Dict) -> List[str]:
    """Check for duplicate input or output files"""
    nodes = dag.get('nodes', [])
    issues = []
    
    inputs = [node['input'] for node in nodes]
    outputs = [node['output'] for node in nodes if node['output']]
    
    # Check duplicate inputs
    seen_inputs = set()
    for inp in inputs:
        if inp in seen_inputs:
            issues.append(f"Duplicate input: {inp}")
        seen_inputs.add(inp)
    
    # Check duplicate outputs
    seen_outputs = set()
    for out in outputs:
        if out in seen_outputs:
            issues.append(f"Duplicate output: {out}")
        seen_outputs.add(out)
    
    return issues

def verify_outputs(dag: Dict, expected_outputs: List[str], not_expected: List[str] = None) -> Tuple[bool, str]:
    """Verify the DAG has expected outputs and doesn't have unexpected ones"""
    actual_outputs = get_output_files(dag)
    
    # Convert expected to normalized paths
    expected_set = {Path(p).as_posix() for p in expected_outputs}
    
    missing = expected_set - actual_outputs
    if missing:
        return False, f"Missing outputs: {missing}"
    
    if not_expected:
        not_expected_set = {Path(p).as_posix() for p in not_expected}
        unwanted = not_expected_set & actual_outputs
        if unwanted:
            return False, f"Unexpected outputs: {unwanted}"
    
    return True, "OK"

def verify_ignored(dag: Dict, should_ignore: List[str]) -> Tuple[bool, str]:
    """Verify certain files are ignored (not in inputs)"""
    actual_inputs = get_input_files(dag)
    
    for ignore_pattern in should_ignore:
        # Check if any input contains this pattern
        for inp in actual_inputs:
            if ignore_pattern in inp:
                return False, f"Should have ignored: {ignore_pattern}"
    
    return True, "OK"

def setup_config1(test_dir: Path):
    """Setup Configuration 1: note.md, file1.md, file2.md, _file3.md, html/index.md"""
    # Clean and create base directory
    config1 = test_dir / 'config1'
    if config1.exists():
        import shutil
        shutil.rmtree(config1)
    config1.mkdir(parents=True, exist_ok=True)
    
    # Create files
    (config1 / 'note.md').write_text('# Note\nNote content')
    (config1 / 'file1.md').write_text('# File 1\nContent 1')
    (config1 / 'file2.md').write_text('# File 2\nContent 2')
    (config1 / '_file3.md').write_text('# File 3\nPrivate file')
    
    # Create html subdirectory with index.md
    (config1 / 'html').mkdir(exist_ok=True)
    (config1 / 'html' / 'index.md').write_text('# Index\nHTML folder index')
    
    # Add some extra files for edge case testing
    (config1 / '.hidden.md').write_text('# Hidden\nHidden file')
    (config1 / 'style.css').write_text('body { margin: 0; }')
    (config1 / 'script.js').write_text('console.log("test");')
    
    return config1

def setup_config2(test_dir: Path):
    """Setup Configuration 2: src1/file1.md, src2/file2.md, html/index.md"""
    # Clean and create base directory
    config2 = test_dir / 'config2'
    if config2.exists():
        import shutil
        shutil.rmtree(config2)
    config2.mkdir(parents=True, exist_ok=True)
    
    # Create directories and files
    (config2 / 'src1').mkdir(exist_ok=True)
    (config2 / 'src1' / 'file1.md').write_text('# Src1 File1\nSource 1 content')
    (config2 / 'src1' / 'style.css').write_text('/* src1 styles */')
    
    (config2 / 'src2').mkdir(exist_ok=True)
    (config2 / 'src2' / 'file2.md').write_text('# Src2 File2\nSource 2 content')
    (config2 / 'src2' / '_draft.md').write_text('# Draft\nDraft content')
    
    (config2 / 'html').mkdir(exist_ok=True)
    (config2 / 'html' / 'index.md').write_text('# HTML Index\nHTML folder index')
    
    # Add .git directory for ignore testing
    (config2 / '.git').mkdir(exist_ok=True)
    (config2 / '.git' / 'config').write_text('git config')
    
    return config2

def setup_config3(test_dir: Path):
    """Setup Configuration 3: Deep nesting and edge cases"""
    config3 = test_dir / 'config3'
    if config3.exists():
        import shutil
        shutil.rmtree(config3)
    config3.mkdir(parents=True, exist_ok=True)
    
    # Deep nesting
    deep = config3 / 'a' / 'b' / 'c' / 'd'
    deep.mkdir(parents=True, exist_ok=True)
    deep.joinpath('deep.md').write_text('# Deep file')
    
    # Various ignore patterns
    (config3 / '_templates').mkdir(exist_ok=True)
    (config3 / '_templates' / 'template.md').write_text('# Template')
    
    (config3 / '.github').mkdir(exist_ok=True)
    (config3 / '.github' / 'workflow.md').write_text('# Workflow')
    
    # Mixed content at root
    (config3 / 'readme.md').write_text('# Readme')
    (config3 / 'README.md').write_text('# README CAPS')  # Test case sensitivity
    
    return config3

def run_test(test_num: int, name: str, args: List[str], cwd: Path, 
             expected_outputs: List[str] = None, 
             not_expected_outputs: List[str] = None,
             should_ignore: List[str] = None,
             should_fail: bool = False,
             error_contains: str = None) -> bool:
    """Run a single test and verify results"""
    print(f"\n{Colors.BOLD}Test {test_num}: {name}{Colors.RESET}")
    print(f"  {Colors.DIM}Directory: {cwd}{Colors.RESET}")
    print(f"  {Colors.DIM}Command: md2html {' '.join(args)}{Colors.RESET}")
    
    success, stdout, stderr = run_md2html(args, cwd=cwd)
    
    # Handle expected failures
    if should_fail:
        if success:
            print(f"  {Colors.RED}✗ Should have failed but succeeded{Colors.RESET}")
            return False
        if error_contains and error_contains.lower() not in stderr.lower():
            print(f"  {Colors.RED}✗ Error message missing '{error_contains}'{Colors.RESET}")
            print(f"  {Colors.DIM}Actual error: {stderr[:100]}{Colors.RESET}")
            return False
        print(f"  {Colors.GREEN}✓ Correctly failed{Colors.RESET}")
        return True
    
    if not success:
        print(f"  {Colors.RED}✗ Command failed{Colors.RESET}")
        if stderr:
            print(f"  {Colors.DIM}Error: {stderr[:200]}{Colors.RESET}")
        return False
    
    dag = parse_dag_output(stdout)
    if not dag:
        print(f"  {Colors.RED}✗ Invalid JSON output{Colors.RESET}")
        return False
    
    nodes = dag.get('nodes', [])
    md_count = sum(1 for n in nodes if n['type'] == 'markdown')
    copy_count = sum(1 for n in nodes if n['type'] == 'copy')
    print(f"  {Colors.DIM}DAG: {len(nodes)} nodes ({md_count} markdown, {copy_count} copy){Colors.RESET}")
    
    # Check for duplicates
    dup_issues = check_for_duplicates(dag)
    if dup_issues:
        print(f"  {Colors.RED}✗ Duplicates found: {dup_issues}{Colors.RESET}")
        return False
    
    # Show actual outputs for debugging (only if there are few)
    actual_outputs = get_output_files(dag)
    if actual_outputs and len(actual_outputs) <= 5:
        print(f"  {Colors.DIM}Outputs: {sorted(actual_outputs)}{Colors.RESET}")
    
    # Verify expected outputs
    if expected_outputs is not None:
        ok, msg = verify_outputs(dag, expected_outputs, not_expected_outputs)
        if not ok:
            print(f"  {Colors.RED}✗ {msg}{Colors.RESET}")
            return False
    
    # Verify ignored files
    if should_ignore:
        ok, msg = verify_ignored(dag, should_ignore)
        if not ok:
            print(f"  {Colors.RED}✗ {msg}{Colors.RESET}")
            return False
    
    print(f"  {Colors.GREEN}✓ Passed{Colors.RESET}")
    return True

def main():
    """Run all tests"""
    print(f"{Colors.BOLD}{Colors.BLUE}=== md2html Comprehensive Test Suite ==={Colors.RESET}")
    
    # Get test directory
    project_root = Path(__file__).parent.parent
    test_dir = project_root / 'tests'
    test_dir.mkdir(exist_ok=True)
    
    # Setup configurations
    print(f"\n{Colors.BOLD}Setting up test configurations...{Colors.RESET}")
    config1_dir = setup_config1(test_dir)
    config2_dir = setup_config2(test_dir)
    config3_dir = setup_config3(test_dir)
    print(f"  ✓ Config 1: {config1_dir}")
    print(f"  ✓ Config 2: {config2_dir}")
    print(f"  ✓ Config 3: {config3_dir}")
    
    passed = 0
    failed = 0
    test_num = 0
    
    # ========== YOUR 10 REQUIRED TESTS ==========
    print(f"\n{Colors.BOLD}{Colors.BLUE}=== Core Required Tests (Config 1) ==={Colors.RESET}")
    print(f"{Colors.DIM}Files: note.md, file1.md, file2.md, _file3.md, html/index.md{Colors.RESET}")
    
    # Test 1: md2html note.md
    test_num += 1
    if run_test(
        test_num,
        "Single file default output",
        ['note.md', '--dry-run'],
        config1_dir,
        expected_outputs=['note.html']
    ):
        passed += 1
    else:
        failed += 1
    
    # Test 2: md2html note.md -o index.html
    test_num += 1
    if run_test(
        test_num,
        "Single file specific output",
        ['note.md', '-o', 'index.html', '--dry-run'],
        config1_dir,
        expected_outputs=['index.html']
    ):
        passed += 1
    else:
        failed += 1
    
    # Test 3: md2html file1.md file2.md -o html/
    test_num += 1
    if run_test(
        test_num,
        "Multiple files to output dir",
        ['file1.md', 'file2.md', '-o', 'html/', '--dry-run'],
        config1_dir,
        expected_outputs=['html/file1.html', 'html/file2.html'],
        not_expected_outputs=['html/html/index.html']
    ):
        passed += 1
    else:
        failed += 1
    
    # Test 4: md2html -r . -o html
    test_num += 1
    if run_test(
        test_num,
        "Recursive current dir to html",
        ['-r', '.', '-o', 'html', '--dry-run'],
        config1_dir,
        expected_outputs=['html/note.html', 'html/file1.html', 'html/file2.html'],
        not_expected_outputs=['html/html/index.html', 'html/_file3.html'],
        should_ignore=['_file3.md', '.hidden.md']
    ):
        passed += 1
    else:
        failed += 1
    
    # Test 5: md2html -r ./* -o html (shell expansion simulation)
    test_num += 1
    items = sorted([str(p.relative_to(config1_dir)) for p in config1_dir.iterdir() if not p.name.startswith('.')])
    if run_test(
        test_num,
        "Recursive glob pattern to html",
        ['-r'] + items + ['-o', 'html', '--dry-run'],
        config1_dir,
        expected_outputs=['html/note.html', 'html/file1.html', 'html/file2.html'],
        not_expected_outputs=['html/html/index.html', 'html/_file3.html'],
        should_ignore=['_file3.md', '.hidden.md']
    ):
        passed += 1
    else:
        failed += 1
    
    # Test 6: md2html html -o html
    test_num += 1
    if run_test(
        test_num,
        "Process html dir to itself",
        ['-r', 'html', '-o', 'html', '--dry-run'],
        config1_dir,
        expected_outputs=['html/index.html']
    ):
        passed += 1
    else:
        failed += 1
    
    print(f"\n{Colors.BOLD}{Colors.BLUE}=== Core Required Tests (Config 2) ==={Colors.RESET}")
    print(f"{Colors.DIM}Files: src1/file1.md, src2/file2.md, html/index.md{Colors.RESET}")
    
    # Test 7: md2html -r src1 src2 -o html/
    test_num += 1
    if run_test(
        test_num,
        "Multiple source dirs to html",
        ['-r', 'src1', 'src2', '-o', 'html/', '--dry-run'],
        config2_dir,
        expected_outputs=['html/src1/file1.html', 'html/src2/file2.html'],
        should_ignore=['_draft.md']
    ):
        passed += 1
    else:
        failed += 1
    
    # Test 8: md2html -r src1 -o html
    test_num += 1
    if run_test(
        test_num,
        "Single source dir to html",
        ['-r', 'src1', '-o', 'html', '--dry-run'],
        config2_dir,
        expected_outputs=['html/file1.html']
    ):
        passed += 1
    else:
        failed += 1
    
    # Test 9: md2html -r src1 src2 -o src1
    test_num += 1
    if run_test(
        test_num,
        "Output to one of input dirs",
        ['-r', 'src1', 'src2', '-o', 'src1', '--dry-run'],
        config2_dir,
        expected_outputs=['src1/src2/file2.html'],
        not_expected_outputs=['src1/src1/file1.html'],
        should_ignore=['_draft.md']
    ):
        passed += 1
    else:
        failed += 1
    
    # Test 10: md2html -o html ./*
    test_num += 1
    items2 = sorted([str(p.relative_to(config2_dir)) for p in config2_dir.iterdir() if not p.name.startswith('.')])
    if run_test(
        test_num,
        "All items with html output",
        ['-r'] + items2 + ['-o', 'html', '--dry-run'],
        config2_dir,
        expected_outputs=['html/src1/file1.html', 'html/src2/file2.html'],
        not_expected_outputs=['html/html/index.html'],
        should_ignore=['.git', '_draft.md']
    ):
        passed += 1
    else:
        failed += 1
    
    # ========== ADDITIONAL EDGE CASE TESTS ==========
    print(f"\n{Colors.BOLD}{Colors.BLUE}=== Additional Edge Case Tests ==={Colors.RESET}")
    
    # Test: CSS/JS files get copied
    test_num += 1
    if run_test(
        test_num,
        "Non-markdown files get copied",
        ['-r', '.', '-o', 'output', '--dry-run'],
        config1_dir,
        expected_outputs=['output/style.css', 'output/script.js', 'output/note.html']
    ):
        passed += 1
    else:
        failed += 1
    
    # Test: Deep nesting
    test_num += 1
    if run_test(
        test_num,
        "Deep directory nesting",
        ['-r', '.', '-o', 'build', '--dry-run'],
        config3_dir,
        expected_outputs=['build/a/b/c/d/deep.html', 'build/readme.html', 'build/README.html'],
        should_ignore=['_templates', '.github']
    ):
        passed += 1
    else:
        failed += 1
    
    # Test: Shell expansion with hidden files (should be excluded by shell)
    test_num += 1
    visible_items = sorted([str(p.relative_to(config1_dir)) 
                           for p in config1_dir.glob('*') 
                           if not p.name.startswith('.') and p.is_file()])  # Only files, not directories
    if run_test(
        test_num,
        "Shell glob * excludes hidden",
        visible_items + ['-o', 'dist', '--dry-run'],
        config1_dir,
        expected_outputs=['dist/note.html', 'dist/file1.html', 'dist/file2.html', 'dist/style.css', 'dist/script.js'],
        should_ignore=['.hidden.md', '_file3.md']
    ):
        passed += 1
    else:
        failed += 1
    
    # ========== ERROR HANDLING TESTS ==========
    print(f"\n{Colors.BOLD}{Colors.BLUE}=== Error Handling Tests ==={Colors.RESET}")
    
    # Test: Non-existent file
    test_num += 1
    if run_test(
        test_num,
        "Non-existent file error",
        ['nonexistent.md', '--dry-run'],
        config1_dir,
        should_fail=True,
        error_contains="does not exist"
    ):
        passed += 1
    else:
        failed += 1
    
    # Test: Directory without recursive flag
    test_num += 1
    if run_test(
        test_num,
        "Directory without -r flag error",
        ['html', '--dry-run'],
        config1_dir,
        should_fail=True,
        error_contains="recursive"
    ):
        passed += 1
    else:
        failed += 1
    
    # Test: Multiple files with single file output
    test_num += 1
    if run_test(
        test_num,
        "Multiple inputs to single file output",
        ['file1.md', 'file2.md', '-o', 'single.html', '--dry-run'],
        config1_dir,
        expected_outputs=['single.html/file1.html', 'single.html/file2.html']
        # This might be unexpected behavior - multiple files force directory interpretation
    ):
        passed += 1
    else:
        failed += 1
    
    # ========== COMPLEX SHELL EXPANSION TESTS ==========
    print(f"\n{Colors.BOLD}{Colors.BLUE}=== Shell Expansion Tests ==={Colors.RESET}")
    
    # Test: *.md pattern
    test_num += 1
    md_files = sorted([str(p.relative_to(config1_dir)) 
                      for p in config1_dir.glob('*.md') 
                      if not p.name.startswith('.')])
    if run_test(
        test_num,
        "*.md pattern",
        md_files + ['-o', 'build', '--dry-run'],
        config1_dir,
        expected_outputs=['build/note.html', 'build/file1.html', 'build/file2.html'],
        should_ignore=['_file3.md', '.hidden.md']  # These shouldn't be in glob results
    ):
        passed += 1
    else:
        failed += 1
    
    # Test: **/*.md pattern (recursive glob)
    test_num += 1
    all_md = sorted([str(p.relative_to(config2_dir)) 
                    for p in config2_dir.glob('**/*.md') 
                    if not any(part.startswith('.') or part.startswith('_') 
                              for part in p.parts)])
    if run_test(
        test_num,
        "**/*.md recursive pattern",
        all_md + ['-o', 'site', '--dry-run'],
        config2_dir,
        expected_outputs=['site/src1/file1.html', 'site/src2/file2.html', 'site/html/index.html']
    ):
        passed += 1
    else:
        failed += 1


        # Test: Single-file mode with ignored patterns (explicit _file3.md)
    test_num += 1
    if run_test(
        test_num,
        "Single file mode processes ignored patterns (_file3.md)",
        ['_file3.md', '--dry-run'],
        config1_dir,
        expected_outputs=['_file3.html']
    ):
        passed += 1
    else:
        failed += 1
    
    # Test: Single file mode with hidden file (.hidden.md)
    test_num += 1
    if run_test(
        test_num,
        "Single file mode processes hidden files (.hidden.md)",
        ['.hidden.md', '--dry-run'],
        config1_dir,
        expected_outputs=['.hidden.html']
    ):
        passed += 1
    else:
        failed += 1
    
    # Test: Single non-MD file with no output dir (expect no node, empty graph)
    test_num += 1
    if run_test(
        test_num,
        "Single non-MD file no output (no action)",
        ['style.css', '--dry-run'],
        config1_dir,
        expected_outputs=[]  # Empty outputs since output == input, skipped
    ):
        passed += 1
    else:
        failed += 1
    
    # Test: Single non-MD file with different output
    test_num += 1
    if run_test(
        test_num,
        "Single non-MD file with output file",
        ['style.css', '-o', 'assets/style.css', '--dry-run'],
        config1_dir,
        expected_outputs=['assets/style.css']
    ):
        passed += 1
    else:
        failed += 1
    
    # Test: Recursive in-place (-o .)
    test_num += 1
    if run_test(
        test_num,
        "Recursive in-place output (-o .)",
        ['-r', '.', '-o', '.', '--dry-run'],
        config1_dir,
        expected_outputs=['note.html', 'file1.html', 'file2.html'],  # MD -> html, non-MD skipped if same
        not_expected_outputs=['style.css', 'script.js'],  # Same input/output, skipped
        should_ignore=['_file3.md', '.hidden.md']
    ):
        passed += 1
    else:
        failed += 1
    
    # Test: Case sensitivity (readme.md and README.md in config3)
    test_num += 1
    if run_test(
        test_num,
        "Case sensitivity handling",
        ['readme.md', 'README.md', '--dry-run'],
        config3_dir,
        expected_outputs=['readme.html', 'README.html']  # Both processed distinctly
    ):
        passed += 1
    else:
        failed += 1
    
    # Test: Invocation from non-CWD (run from project_root, assuming config1_dir is tests/config1)
    test_num += 1
    if run_test(
        test_num,
        "Invocation from parent directory",
        ['-r', 'tests/config1/html', '-o', 'tests/config1/output', '--dry-run'],
        project_root,  # Note: Use project_root as cwd here
        expected_outputs=['tests/config1/output/index.html']
    ):
        passed += 1
    else:
        failed += 1
    
    # Test: No args (expect error)
    test_num += 1
    if run_test(
        test_num,
        "No input args error",
        ['--dry-run'],
        config1_dir,
        should_fail=True,
        error_contains="No input files"
    ):
        passed += 1
    else:
        failed += 1
    
    # Test: Directory with only ignored files
    test_num += 1
    if run_test(
        test_num,
        "Recursive dir with only ignored files",
        ['-r', '_templates', '--dry-run'],
        config3_dir,
        expected_outputs=[]  # Empty graph
    ):
        passed += 1
    else:
        failed += 1
    
    # Test: Duplicate inputs error
    test_num += 1
    if run_test(
        test_num,
        "Duplicate input files error",
        ['note.md', 'note.md', '--dry-run'],
        config1_dir,
        should_fail=True,  # Errors on add_node
        error_contains="already exists"
    ):
        passed += 1
    else:
        failed += 1
    
    # Bonus: Add a file with space for testing (setup in config1)
    (config1_dir / 'file with space.md').write_text('# File with space')
    test_num += 1
    if run_test(
        test_num,
        "File with spaces in name",
        ['file with space.md', '--dry-run'],
        config1_dir,
        expected_outputs=['file with space.html']
    ):
        passed += 1
    else:
        failed += 1
    
    # ========== SUMMARY ==========
    print(f"\n{Colors.BOLD}{'='*60}{Colors.RESET}")
    print(f"{Colors.BOLD}Test Results Summary:{Colors.RESET}")
    print(f"  {Colors.GREEN}Passed: {passed}{Colors.RESET}")
    if failed > 0:
        print(f"  {Colors.RED}Failed: {failed}{Colors.RESET}")
    
    total = passed + failed
    percentage = (passed / total * 100) if total > 0 else 0
    
    if failed == 0:
        print(f"\n{Colors.GREEN}{Colors.BOLD}✓ All {total} tests passed!{Colors.RESET}")
    else:
        print(f"\n{Colors.YELLOW}{Colors.BOLD}⚠ {failed}/{total} tests failed ({percentage:.1f}% pass rate){Colors.RESET}")
    
    print(f"\n{Colors.DIM}Test files preserved in:{Colors.RESET}")
    print(f"  • {config1_dir}")
    print(f"  • {config2_dir}")
    print(f"  • {config3_dir}")
    print(f"{Colors.DIM}You can manually inspect and re-run individual tests.{Colors.RESET}")
    
    return 0 if failed == 0 else 1

if __name__ == "__main__":
    sys.exit(main())