#!/usr/bin/env python3
"""
File path and DAG generation tests for md2html
Generated by claude opus 4.1
"""

import json
import shutil
from pathlib import Path
from typing import Dict, List, Set, Tuple

from .testsuite import TestContext, run_command

def parse_dag(output: str) -> Dict:
    """Parse the JSON DAG output"""
    try:
        return json.loads(output)
    except json.JSONDecodeError:
        return None

def get_outputs(dag: Dict) -> Set[str]:
    """Get set of output files from DAG"""
    nodes = dag.get('nodes', [])
    outputs = set()
    for node in nodes:
        if node['output']:
            outputs.add(Path(node['output']).as_posix())
    return outputs

def get_inputs(dag: Dict) -> Set[str]:
    """Get set of input files from DAG"""
    nodes = dag.get('nodes', [])
    inputs = set()
    for node in nodes:
        inputs.add(Path(node['input']).as_posix())
    return inputs

def check_duplicates(dag: Dict) -> List[str]:
    """Check for duplicate input or output files"""
    nodes = dag.get('nodes', [])
    issues = []

    inputs = [node['input'] for node in nodes]
    outputs = [node['output'] for node in nodes if node['output']]

    seen_inputs = set()
    for inp in inputs:
        if inp in seen_inputs:
            issues.append(f"Duplicate input: {inp}")
        seen_inputs.add(inp)

    seen_outputs = set()
    for out in outputs:
        if out in seen_outputs:
            issues.append(f"Duplicate output: {out}")
        seen_outputs.add(out)

    return issues

def verify_outputs(dag: Dict, expected: List[str], not_expected: List[str] = None) -> Tuple[bool, str]:
    """Verify the DAG has expected outputs and doesn't have unexpected ones"""
    actual = get_outputs(dag)

    expected_set = {Path(p).as_posix() for p in expected}

    missing = expected_set - actual
    if missing:
        return False, f"Missing outputs: {missing}"

    if not_expected:
        not_expected_set = {Path(p).as_posix() for p in not_expected}
        unwanted = not_expected_set & actual
        if unwanted:
            return False, f"Unexpected outputs: {unwanted}"

    return True, "OK"

def verify_ignored(dag: Dict, should_ignore: List[str]) -> Tuple[bool, str]:
    """Verify certain files are ignored (not in inputs)"""
    actual_inputs = get_inputs(dag)

    for ignore_pattern in should_ignore:
        for inp in actual_inputs:
            if ignore_pattern in inp:
                return False, f"Should have ignored: {ignore_pattern}"

    return True, "OK"

def test_dag_command(ctx: TestContext, name: str, args: List[str], cwd: Path,
                     expected_outputs: List[str] = None,
                     not_expected: List[str] = None,
                     should_ignore: List[str] = None,
                     should_fail: bool = False,
                     error_contains: str = None) -> bool:
    """
    Test md2html command that outputs a DAG as JSON.
    This is specific to the filepath test suite.
    """
    ctx.test_start(name)
    ctx.detail(f"Directory: {cwd}")
    ctx.detail(f"Command: md2html {' '.join(args)}")

    # Run the command
    success, stdout, stderr = run_command(args, cwd)

    # Handle expected failures
    if should_fail:
        if success:
            return ctx.fail_test("Should have failed but succeeded")
        if error_contains and error_contains.lower() not in stderr.lower():
            ctx.detail(f"Actual error: {stderr[:100]}")
            return ctx.fail_test(f"Error message missing '{error_contains}'")
        return ctx.pass_test("Correctly failed")

    # Handle unexpected failures
    if not success:
        if stderr:
            ctx.detail(f"Error: {stderr[:200]}")
        return ctx.fail_test("Command failed")

    # Parse and validate output
    dag = parse_dag(stdout)
    if not dag:
        return ctx.fail_test("Invalid JSON output")

    # Print DAG info
    nodes = dag.get('nodes', [])
    md_count = sum(1 for n in nodes if n['type'] == 'markdown')
    copy_count = sum(1 for n in nodes if n['type'] == 'copy')
    ctx.detail(f"DAG: {len(nodes)} nodes ({md_count} markdown, {copy_count} copy)")

    # Check for duplicates
    dup_issues = check_duplicates(dag)
    if dup_issues:
        return ctx.fail_test(f"Duplicates found: {dup_issues}")

    # Show outputs if not too many
    actual_outputs = get_outputs(dag)
    if actual_outputs and len(actual_outputs) <= 5:
        ctx.detail(f"Outputs: {sorted(actual_outputs)}")

    # Verify expected outputs
    if expected_outputs is not None:
        ok, msg = verify_outputs(dag, expected_outputs, not_expected)
        if not ok:
            return ctx.fail_test(msg)

    # Verify ignored files
    if should_ignore:
        ok, msg = verify_ignored(dag, should_ignore)
        if not ok:
            return ctx.fail_test(msg)

    return ctx.pass_test()

def run_filepath_tests(ctx: TestContext) -> Tuple[int, int]:
    """Run all file path tests and return (passed, failed) counts"""

    # Setup test directory
    project_root = Path(__file__).parent.parent
    test_dir = project_root / 'tests'
    test_dir.mkdir(exist_ok=True)

    # === Setup Config 1 ===
    ctx.print("Setting up test configurations...", level='verbose')
    config1 = test_dir / 'config1'
    if config1.exists():
        shutil.rmtree(config1)
    config1.mkdir(parents=True, exist_ok=True)

    # Create files
    (config1 / 'note.md').write_text('# Note\nNote content')
    (config1 / 'file1.md').write_text('# File 1\nContent 1')
    (config1 / 'file2.md').write_text('# File 2\nContent 2')
    (config1 / '_file3.md').write_text('# File 3\nPrivate file')
    (config1 / 'html').mkdir(exist_ok=True)
    (config1 / 'html' / 'index.md').write_text('# Index\nHTML folder index')
    (config1 / '.hidden.md').write_text('# Hidden\nHidden file')
    (config1 / 'style.css').write_text('body { margin: 0; }')
    (config1 / 'script.js').write_text('console.log("test");')

    # === Setup Config 2 ===
    config2 = test_dir / 'config2'
    if config2.exists():
        shutil.rmtree(config2)
    config2.mkdir(parents=True, exist_ok=True)

    (config2 / 'src1').mkdir(exist_ok=True)
    (config2 / 'src1' / 'file1.md').write_text('# Src1 File1\nSource 1 content')
    (config2 / 'src1' / 'style.css').write_text('/* src1 styles */')

    (config2 / 'src2').mkdir(exist_ok=True)
    (config2 / 'src2' / 'file2.md').write_text('# Src2 File2\nSource 2 content')
    (config2 / 'src2' / '_draft.md').write_text('# Draft\nDraft content')

    (config2 / 'html').mkdir(exist_ok=True)
    (config2 / 'html' / 'index.md').write_text('# HTML Index\nHTML folder index')

    (config2 / '.git').mkdir(exist_ok=True)
    (config2 / '.git' / 'config').write_text('git config')

    # === Setup Config 3 ===
    config3 = test_dir / 'config3'
    if config3.exists():
        shutil.rmtree(config3)
    config3.mkdir(parents=True, exist_ok=True)

    deep = config3 / 'a' / 'b' / 'c' / 'd'
    deep.mkdir(parents=True, exist_ok=True)
    deep.joinpath('deep.md').write_text('# Deep file')

    (config3 / '_templates').mkdir(exist_ok=True)
    (config3 / '_templates' / 'template.md').write_text('# Template')

    (config3 / '.github').mkdir(exist_ok=True)
    (config3 / '.github' / 'workflow.md').write_text('# Workflow')

    (config3 / 'readme.md').write_text('# Readme')
    (config3 / 'README.md').write_text('# README CAPS')

    ctx.detail(f"✓ Config 1: {config1}")
    ctx.detail(f"✓ Config 2: {config2}")
    ctx.detail(f"✓ Config 3: {config3}")

    # === Core Required Tests (Config 1) ===
    ctx.print_header("Core Required Tests (Config 1)")
    ctx.print("Files: note.md, file1.md, file2.md, _file3.md, html/index.md", level='verbose')

    # Test 1: Single file default output
    test_dag_command(ctx, "Single file default output",
                     ['note.md', '--dry-run'], config1,
                     expected_outputs=['note.html'])

    # Test 2: Single file specific output
    test_dag_command(ctx, "Single file specific output",
                     ['note.md', '-o', 'index.html', '--dry-run'], config1,
                     expected_outputs=['index.html'])

    # Test 3: Multiple files to output dir
    test_dag_command(ctx, "Multiple files to output dir",
                     ['file1.md', 'file2.md', '-o', 'html/', '--dry-run'], config1,
                     expected_outputs=['html/file1.html', 'html/file2.html'],
                     not_expected=['html/html/index.html'])

    # Test 4: Recursive current dir to html
    test_dag_command(ctx, "Recursive current dir to html",
                     ['-r', '.', '-o', 'html', '--dry-run'], config1,
                     expected_outputs=['html/note.html', 'html/file1.html', 'html/file2.html'],
                     not_expected=['html/html/index.html', 'html/_file3.html'],
                     should_ignore=['_file3.md', '.hidden.md'])

    # Test 5: Recursive glob pattern
    items = sorted([str(p.relative_to(config1)) for p in config1.iterdir()
                   if not p.name.startswith('.')])
    test_dag_command(ctx, "Recursive glob pattern to html",
                     ['-r'] + items + ['-o', 'html', '--dry-run'], config1,
                     expected_outputs=['html/note.html', 'html/file1.html', 'html/file2.html'],
                     not_expected=['html/html/index.html', 'html/_file3.html'],
                     should_ignore=['_file3.md', '.hidden.md'])

    # Test 6: Process html dir to itself
    test_dag_command(ctx, "Process html dir to itself",
                     ['-r', 'html', '-o', 'html', '--dry-run'], config1,
                     expected_outputs=['html/index.html'])

    # === Core Required Tests (Config 2) ===
    ctx.print_header("Core Required Tests (Config 2)")
    ctx.print("Files: src1/file1.md, src2/file2.md, html/index.md", level='verbose')

    # Test 7: Multiple source dirs to html
    test_dag_command(ctx, "Multiple source dirs to html",
                     ['-r', 'src1', 'src2', '-o', 'html/', '--dry-run'], config2,
                     expected_outputs=['html/src1/file1.html', 'html/src2/file2.html'],
                     should_ignore=['_draft.md'])

    # Test 8: Single source dir to html
    test_dag_command(ctx, "Single source dir to html",
                     ['-r', 'src1', '-o', 'html', '--dry-run'], config2,
                     expected_outputs=['html/file1.html'])

    # Test 9: Output to one of input dirs
    test_dag_command(ctx, "Output to one of input dirs",
                     ['-r', 'src1', 'src2', '-o', 'src1', '--dry-run'], config2,
                     expected_outputs=['src1/src2/file2.html'],
                     not_expected=['src1/src1/file1.html'],
                     should_ignore=['_draft.md'])

    # Test 10: All items with html output
    items2 = sorted([str(p.relative_to(config2)) for p in config2.iterdir()
                    if not p.name.startswith('.')])
    test_dag_command(ctx, "All items with html output",
                     ['-r'] + items2 + ['-o', 'html', '--dry-run'], config2,
                     expected_outputs=['html/src1/file1.html', 'html/src2/file2.html'],
                     not_expected=['html/html/index.html'],
                     should_ignore=['.git', '_draft.md'])

    # === Additional Edge Case Tests ===
    ctx.print_header("Additional Edge Case Tests")

    # CSS/JS copy test
    test_dag_command(ctx, "Non-markdown files get copied",
                     ['-r', '.', '-o', 'output', '--dry-run'], config1,
                     expected_outputs=['output/style.css', 'output/script.js', 'output/note.html'])

    # Deep nesting test
    test_dag_command(ctx, "Deep directory nesting",
                     ['-r', '.', '-o', 'build', '--dry-run'], config3,
                     expected_outputs=['build/a/b/c/d/deep.html', 'build/readme.html', 'build/README.html'],
                     should_ignore=['_templates', '.github'])

    # Add file with spaces dynamically
    (config1 / 'file with space.md').write_text('# File with space')
    test_dag_command(ctx, "File with spaces in name",
                     ['file with space.md', '--dry-run'], config1,
                     expected_outputs=['file with space.html'])

    # Test something else that's not DAG-related
    ctx.test_start("Help text contains usage")
    success, stdout, stderr = run_command(['--help'], config1)
    if success and "Usage:" in stdout:
        ctx.pass_test()
    else:
        ctx.fail_test("Help text missing usage info")

    # Single file mode with ignored patterns
    test_dag_command(ctx, "Single file mode processes ignored patterns",
                     ['_file3.md', '--dry-run'], config1,
                     expected_outputs=['_file3.html'])

    # Single file mode with hidden file
    test_dag_command(ctx, "Single file mode processes hidden files",
                     ['.hidden.md', '--dry-run'], config1,
                     expected_outputs=['.hidden.html'])

    # === Error Handling Tests ===
    ctx.print_header("Error Handling Tests")

    # Non-existent file
    test_dag_command(ctx, "Non-existent file error",
                     ['nonexistent.md', '--dry-run'], config1,
                     should_fail=True, error_contains="does not exist")

    # Directory without recursive flag
    test_dag_command(ctx, "Directory without -r flag error",
                     ['html', '--dry-run'], config1,
                     should_fail=True, error_contains="recursive")

    # No args
    test_dag_command(ctx, "No input args error",
                     ['--dry-run'], config1,
                     should_fail=True, error_contains="No input files")

    # Duplicate inputs
    test_dag_command(ctx, "Duplicate input files error",
                     ['note.md', 'note.md', '--dry-run'], config1,
                     should_fail=True, error_contains="already exists")

    # === Shell Expansion Pattern Tests ===
    ctx.print_header("Shell Expansion Tests")

    # *.md pattern
    md_files = sorted([str(p.relative_to(config1)) for p in config1.glob('*.md')
                      if not p.name.startswith('.')])
    test_dag_command(ctx, "*.md pattern",
                     md_files + ['-o', 'build', '--dry-run'], config1,
                     expected_outputs=['build/note.html', 'build/file1.html', 'build/file2.html'],
                     should_ignore=['_file3.md', '.hidden.md'])

    # **/*.md recursive pattern
    all_md = sorted([str(p.relative_to(config2)) for p in config2.glob('**/*.md')
                    if not any(part.startswith('.') or part.startswith('_')
                              for part in p.parts)])
    test_dag_command(ctx, "**/*.md recursive pattern",
                     all_md + ['-o', 'site', '--dry-run'], config2,
                     expected_outputs=['site/src1/file1.html', 'site/src2/file2.html', 'site/html/index.html'])

    # In-place conversion
    test_dag_command(ctx, "Recursive in-place output",
                     ['-r', '.', '-o', '.', '--dry-run'], config1,
                     expected_outputs=['note.html', 'file1.html', 'file2.html'],
                     not_expected=['style.css', 'script.js'],
                     should_ignore=['_file3.md', '.hidden.md'])

    # Case sensitivity
    test_dag_command(ctx, "Case sensitivity handling",
                     ['readme.md', 'README.md', '--dry-run'], config3,
                     expected_outputs=['readme.html', 'README.html'])

    # Parent directory invocation
    test_dag_command(ctx, "Invocation from parent directory",
                     ['-r', 'tests/config1/html', '-o', 'tests/config1/output', '--dry-run'],
                     project_root,
                     expected_outputs=['tests/config1/output/index.html'])

    # Add another file dynamically to show we can
    (config1 / 'dynamic.md').write_text('# Dynamically added')
    test_dag_command(ctx, "Dynamically added file",
                     ['dynamic.md', '--dry-run'], config1,
                     expected_outputs=['dynamic.html'])

    # Different type of test - just running commands directly
    ctx.test_start("Version flag works")
    success, stdout, stderr = run_command(['--version'], config1)
    # For now we expect this to fail since --version isn't implemented
    if not success:
        ctx.pass_test("Version not implemented yet")
    else:
        ctx.pass_test("Has version info")

    # Cleanup
    if not ctx.keep_files:
        shutil.rmtree(test_dir)
        ctx.print(f"\nTest files cleaned up", level='verbose')
    else:
        ctx.print(f"\nTest files preserved in:", level='verbose')
        ctx.print(f"  • {config1}", level='verbose')
        ctx.print(f"  • {config2}", level='verbose')
        ctx.print(f"  • {config3}", level='verbose')

    return ctx.passed, ctx.failed
